[obligation=informative]
:sectnums:
== Evaluation of GFMs for satellite imagery 
// Overviewとかの方がいい？
The most common method to evaluate performance of GFM is to use downstream task. +
This section introduces representative examples of models, tasks, metrics, dataset and the evaluation steps. +
Although research on multimodal models has been growing recently, we only focus vision-only tasks.

=== Models, Tasks, Metrics, Datasets
This section describes representative models, tasks, metrics and datasets. +

==== Models
// *_Models_* +
The following table introduces representative models released in 2024 and later.

// .Representative GFMs
[%unnumbered]
|===
|Model name |Backborn |Training methods |Pretrain images |Description

|<<THOR, THOR>> |transformer-based |Contrastive learning |Sentinel-1, Sentinel-2, Sentinel-3 |First architecture to unify data from Sentinel-1, -2, and -3 (OLCI & SLSTR) satellites using framework that extends the flexible patching to a MAE setup.
|<<Prithvi-EO-2.0, Prithvi-EO-2.0>> |transformer-based |MAE |Landsat 8, 9 and Sentinel-2 |A multitemporal model trained on global time series data that incorporates temporal and location embeddings, with 600M parameters which is among the largest in the field of EO.
|<<DOFA, DOFA>> |transformer-based |MIM |Sentinel-1, Sentinel-2, Gaofen-2, NAIP, EnMAP |A multimodal model leverages wavelength as a unifying parameter across various EO modalities.
|===

==== Tasks
// *_Tasks_* +
The following table describes typical EO downstream tasks used to evaluate GFMs. +
As mentioned above, we do not include vision-language tasks such as caption generation or counting.
// |===
// |Task name |Description 
// |Semantic segmentation |Semantic segmentation aims to assign a class label to each pixel in a satellite image.
// |Instance segmentation |The aim of instance segmentation is to detect each object in an image by predicting its category, bounding box and a pixel-level mask.
// |Change detection |Change detection identifies differences at pixel-level between time-series images. This task is used for monitoring land use changes and disaster impacts.
// |Regression |Regression estimates continuous variables from satellite imagery, such as forest aboveground biomass, surface temperature or elevation. 
// |Scene classification |While semantic segmentation assigns a label to pixel-level, scene classification focuses on image-level.
// |Object detection |Object detection aims to identify and localize objects, such as buildings, vehicles and ships.
// |===

// ISOを引用した方がこちら↓
// .Representative tasks
[%unnumbered]
|===
|Task name |Description 

|Semantic segmentation |task of assigning a class label to each pixel in a satellite image.
|Change detection |recognition of changes between images acquired at different times. <<ISO19178-1, ISO 19178-1:2025>>
|Regression |task of estimating continuous variables from satellite imagery, such as forest aboveground biomass, surface temperature or elevation. 
|Scene classification |task of identifying scene categories of images, on the basis of a training set of images whose scene categories are known. <<ISO19178-1, ISO 19178-1:2025>>
|Object detection |recognition of objects from images. <<ISO19178-1, ISO 19178-1:2025>>
|===

// Scene classification: 画像のクラスラベル
// Object detection: 個々の物体（クラス + 位置（BBox））

==== Metrics
// *_Metrics_* +
The following table describes representative metrics and its definition.

// .Representative metrics
[%unnumbered]
|===
|Metric name |Description |Task commonly used

|mIoU 
|The average IoU across all classes. (IoU is defined as the number of correctly predicted labels divided by the union of predicted and true labels. <<ISO/IEC_4213, ISO/IEC CD 4213:2025>>)
|Semantic segmentation, Change detection
|mAP
|mAP averages precision scores across classes to reflect both classification and localization accuracy. (Precision of a class is the proportion of samples correctly classified as positive for that class out of all samples classified as positive. <<ISO/IEC_4213, ISO/IEC CD 4213:2025>>)
|Object detection
//Instance segmentation, 
|F1 
|F1 score, also known as F-score, F-measure or F1-measure, is the harmonic mean of precision and recall with equal weights. <<ISO/IEC_4213, ISO/IEC CD 4213:2025>> 
|Change detection, Scene classification (multi label)
|mF1 
|The average F1 across all classes which is critical for high-precision tasks like building or road segmentation.
|Semantic segmentation
|Accuracy 
|Accuracy is the ratio of correct predictions to all predictions. <<ISO/IEC_4213, ISO/IEC CD 4213:2025>> 
|Scene classification (single label)
|RMSE 
|RMSE compares predicted and true values using the square root of MSE. Lower values of RMSE represent more accurate predictions. (MSE measures the average squared difference between estimated and true values. <<ISO/IEC_4213, ISO/IEC CD 4213:2025>>)
|Regression
|===

// |===
// |Metric name |Description |Task commonly used
// |Jaccard index (IoU)
// |IoU is defined as the number of correctly predicted labels divided by the union of predicted and true labels. <<ISO/IEC_4213, ISO/IEC CD 4213:2025>> 
// |Semantic segmentation, Change detection
// |Precision 
// |Precision of a class is the proportion of samples correctly classified as positive for that class out of all samples classified as positive. As precision increases, more true positives are detected, but false negatives are not accounted for. <<ISO/IEC_4213, ISO/IEC CD 4213:2025>> 
// |Change detection
// |Recall 
// |Recall of a class is the proportion of samples correctly classified as positive for that class out of all positive samples. As recall increases, more true positives are detected, but false positives are not accounted for. <<ISO/IEC_4213, ISO/IEC CD 4213:2025>> 
// |Change detection
// |F1 
// |F1 score, also known as F-score, F-measure or F1-measure, is the harmonic mean of precision and recall with equal weights. <<ISO/IEC_4213, ISO/IEC CD 4213:2025>> 
// |Change detection,  Multilabel classification
// |mIoU 
// |The average IoU across all classes.
// |Semantic segmentation, Change detection
// |mAP
// |mAP averages precision scores across classes to reflect both classification and localization accuracy
// |Scene classification, Object detection
// |mF1 
// |The average F1 across all classes which is critical for high-precision tasks like building or road segmentation.
// |Semantic segmentation, Change detection
// |Accuracy 
// |Accuracy is the ratio of correct predictions to all predictions. <<ISO/IEC_4213, ISO/IEC CD 4213:2025>> 
// |Scene classification
// |RMSE 
// |RMSE compares predicted and true values using the square root of MSE. Lower values of RMSE represent more accurate predictions. (MSE measures the average squared difference between estimated and true values.) <<ISO/IEC_4213, ISO/IEC CD 4213:2025>> 
// |Regression
// |===

==== Datasets
// *_Datasets_* +
The following table describes some popular datasets to evaluate downstream tasks.

// .Representative datasets
[%unnumbered]
|===
|Datasets name |Task to evaluate |Modalities |Benchmark using this dataset

|<<SpaceNet, SpaceNet>> |Semantic segmentation, Change detection |Planet |<<PANGAEA, PANGAEA>>, <<GEO-Bench-2, GEO-Bench-2>>
|<<BioMassters, BioMassters>> |Regression |Sentinel-1, Sentinel-2 |<<PANGAEA, PANGAEA>>, <<GEO-Bench-2, GEO-Bench-2>>
|<<So2Sat, So2Sat>> |Scene classification |Sentinel-1, Sentinel-2 |<<GEO-Bench, GEO-Bench>>, <<GEO-Bench-2, GEO-Bench-2>>
|<<EverWatch, EverWatch>> |Object detection |Aerial RGB |<<GEO-Bench-2, GEO-Bench-2>>
|===
// |===
// |Datasets name |Task to evaluate |Modalities |GSD |Benchmark using this dataset
// |<<SpaceNet7>> |Semantic segmentation, Change detection |Planet |3m |<<PANGAEA>>, <<GEO-Bench-2>>
// |<<BioMassters>> |Regression |Sentinel-1, Sentinel-2 |10m |<<PANGAEA>>, <<GEO-Bench-2>>
// |<<So2Sat>> |Scene classification |Sentinel-1, Sentinel-2 |10m |<<GEO-Bench>>, <<GEO-Bench-2>>
// |<<EverWatch>> |Object detection |Aerial RGB |0.1m |<<GEO-Bench-2>>
// |===

=== Evaluation process
==== Evaluation scope
First, an evaluator needs to determine the purpose of the experiments and scope. +
For example:

* clarify the objectives of experiments and which performance criteria is most appropriate (e.g., evaluate score, inference time, robustness across modalities)
* select baseline model to compare performance

==== Dataset selection
The second step is to select a dataset or datasets for downstream. +
Based on the objective and scope, the evaluator can choose from perspectives such as task, temporality and modality.
The temporality and modality between the downstream datasets and models do not need to be perfectly aligned, as this can be handled during model adaptation.

==== Data adaptation
When image size or data distribution is different between pretrain dataset and downstream dataset, cropping, resizing and normalization are needed. +
If downstream dataset are scarce or lack diversity, data augmentation including horizontal and vertical flipping is important as it helps 
improve generalization.

==== Model adaptation
Model adaptation defines how to adjust GFMs to assess its performance on downstram tasks.

*_Training strategy_* +
The evaluator needs to select a method to adapt a foundation model for downstream task. +
The encoder weight can be used either frozen or updated during training. +
Using frozen weights is a common evaluation protocol in self-supervised learning. 
However, this approach can be highly dependent on the pre-training task and may fail to encode information important for the downstream task.
On the other hand, it is reported that fine-tuning often mitigates this issue and frequently demonstrate much higher generalization performance. +
In practice, <<PANGAEA, PANGAEA>> adapts frozen encoder and <<GEO-Bench-2, GEO-Bench-2>> uses fine-tuning strategy. 
// decoderの扱いはtaskによって異なるケースが多い。
// ・Classification: linear 
// ・segmentation and regression: plug UPerNet, UNet decoder
// ・Object detection, Instance segmentation:
Depending on the task, we attach a task-specific head to the encoder, such as a linear classifier for image classification, 
a UNet or UPerNet decoder for pixel-wise prediction tasks (e.g., semantic segmentation and regression), 
and a FPN-based detection head for object detection.
// and instance segmentation

*_Modality handling_* +
// GFM は特定のセンサ／バンド構成で事前学習されているが、下流タスクのデータセットとは バンド数・種類が一致しないことが多い。
// そのためこのステップが必要。
// PANGEAはband match or zero-padding、GEO-Gench-2はセンサごとに対応を分けている。
This step is specific to the remote sensing domain, because GFMs are pre-trained with multiple sensor-band that often differ from the bands available 
in downstream datasets. +
One approach suggested in <<PANGAEA, PANGAEA>> is to use only overlapping bands and zero-padding missing ones in downstream task 
(e.g., if a model is pre-trained on RGB images and the downstream dataset consists of Sentinel-2 image, we use only the RGB bands from Sentinel-2). +
On the other hand, <<GEO-Bench-2, GEO-Bench-2>> offers modality-specific handling. For multispectral bands, it is requested to map model bands to the dataset bands with the nearest wavelength 
(e.g., if a model pre-trained with only Sentinel-2 Band 4, we select the dataset band whose spectral range is closest to the red band). 
For SAR, if a model cannot process radar data natively, we adapt the input by loading the VV and VH polarization bands into an RGB-like three-channel format 
(e.g., using VV, VH, and VV as the model's R, G, and B channels, repeating VV to match the bands size).

*_Temporality handling_* +
// すべてのGFMがtemporalityを扱っている訳ではない。モデルがマルチテンポラルに対応している場合は、加工なしでdownstream datasetを入力する。
// 対応しておらず、downstream datasetがマルチテンポラルの場合は次元を揃える方法がある。
// 例えばencoderに1枚ずつ画像を入力し、encoderのoutputをconcatする方法だ。もしくは時系列次元が大きい場合などはoutputを平均したり、L-TAE, linear層を使う方法がある。
Not all GFMs are pre-trained using multitemporal datasets. +
If a model natively supports multitemporal, multitemporal downstream datasets can be fed directly. 
However, to enable models that can only handle single images to be trained on multitemporal datasets, each timestamp is passed independently through 
the encoder and the output features are aggregated over time before passing decoder. 
Common temporal aggregation methods include concatenation, mean pooling, a naive linear mapping that compresses the temporal dimension and 
temporal-attention modules such as L-TAE suggested by <<LTAE, Vivien Sainte Fare Garnot and Loic Landrieu>>.

==== Training and evaluating
// ・学習にあたって、lr、batch_size,epoch数、weghti decayなどのhyperparameterを設定する必要があります
// ・探索を行う場合はフェアな評価のため探索範囲を固定します
// ・これらは最大何トライアルまでかを固定し、ばらつきを考慮するためRepeated Testの回数も統一します。
For a fair comparison, the model shoud be fine-tuned with an identical set of hyperparameters such as learning rate, batch size, number of epochs 
and weight decay. 
For hyperparameter optimization, the search space and the maximum number of trials should be fixed. +
The models are fine-tuned using the specified hyperparameter values and then evaluted with metrics. It is recomended to repeat the fine-tuning process 
a few times with multiple random seeds to account for training stochasticity. +
As evaluation usually conducted with multiple datasets, a methodology for aggregating scores is proposed, which normalize each score, apply bootstrapping, aggregate using the interquartile mean (IQM) and then compute the final averaged score. (<<GEO-Bench-2, GEO-Bench-2>>)

=== Challenges and future directions
// ①汎化性能ない
// GFMは汎化性能が不足している。 
// RSデータは地域・時間・センサなどが異なるとデータ分布が大きく異なる。 
// 本来その多様性に対応できるようなモデルが必要だが（e.g., Europeにも南米にも対応できるモデル、Landsat-8にもSentinel-2にも対応できるモデル）、trainingが偏っており、ドメインシフトが起こってしまうのが現状。 そしてそれを評価するための指標がない。
// →(1) datasetをセンサ、地域など複数の視点で評価するしくみを確立する（地域レベルではうまく評価できているけど、センサレベルではうまくできていないなど）
// →(2) Optical ↔ SARなどドメインが異なるもの同士の距離を定量化し、どのデータをどれだけ学習に使うべきかを決める（Data Scheduling）
// ②ブラックボックス化
// GFMは環境問題など重要なタスクで利用されているため、信頼性、透明性、説明責任を確保するために、それらの解釈可能性を高めることが不可欠である。
// しかし説明性がないのが現状であり、異なるmodalityの情報を統合するときにこの課題は顕著になる（それはmodalityの情報を融合するステップにおいて、どのモダリティの情報をどれだけ信頼したのか？意思決定に働いたのか？が不透明だから）
// →(1)説明可能なAI（XAI）をRS分野でも活用し、モデルの出力を、地理的根拠（どこ）と物理的根拠（どんな実データ/観測値に基づいて判断したのか？）に結びつけて説明できるようにする

// ★もしevaluationに限って書く場合
// ①uncertaintyが評価できていない（モデルの予測にどれくらい自信があるか、未知のドメインにどれくらい適応できるかや画像のノイズなどによる影響）https://www.codemajin.net/uncertainty-in-machine-learning/。これは実世界での応用で不可欠。
// ②リソースに制限される可能性がある。評価時にはfine-tuningを行うがEOデータは高画質の場合もあり、大きなリソースが必要。

As introduced, studies on GFMs for satellite imagery are rapidly advancing.
However the following limitations have been identified, as reported in <<GEO-Bench-2>>, <<Survey>> and <<Foundation>>. +

*_Uncertainty evaluation_* +
GFMs still struggle with generalization across domains. EO data are highly heterogeneous because they are collected from different sensors, regions and temporality. 
This heterogeneity leads to substantial shifts in data distribution across domains, meaning that the statistical properties of the training data often differ 
from those of the test data. Such domain shifts make it hard for the model to learn stable features, which lowers its generalization performance.
However, existing evaluations lack standardized protocols for assessing predictive uncertainty. +
One idea to address this issue is to establish hierarchical evaluation frameworks. They assess datasets across multiple aspects (e.g., region, sensor) to reveal where coverage or bias is insufficient.

*_Interpretability evaluation_* +
EO data are used in the areas such as environmental monitoring, climate change assessment and disaster response, where decisions can have serious societal and ecological impacts. 
Because of this, these applications require transparency, traceability, robustness.
However, GFMs still provide limited interpretability. This problem becomes even more significant when they integrate heterogeneous sensor modalities, 
because the fusion process is complex and the internal reasoning steps are hard to visualize or interpret. +
A promising direction for improving interpretability is applying explainable AI in remote sensing, enabling users to connect model outputs with geospatial evidence 
(e.g., the specific pixels/regions responsible for the output) and physical evidence (e.g., thermal radiance).