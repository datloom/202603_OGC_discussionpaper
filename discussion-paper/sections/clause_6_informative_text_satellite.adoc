[obligation=informative]
:sectnums:
== Evaluation of GFMs for Satellite Imagery 
// Overviewとかの方がいい？
The most common method to evaluate performance of GFM is to use downstream task. +
This section introduces representative examples of models, tasks, metrics, dataset and the evaluation steps.

=== Models, Tasks, Metrics, Datasets
*Models* +
This section introduces representative GFMs.
|===
|Model Name |Backborn |Training Methods
|<<TerraMind>> |Transformer-based |
|<<Prithvi-EO-2.0>> |ViT |Masked Image Modeling
|<<AnySat>> |ViT |Contrastive Learning
|===

*Tasks* +
This section describes typical EO downstream tasks used to evaluate GFMs. +
We do not include vision-language tasks such as Caption Generation or Counting.
|===
|Task Name |Description 
|Semantic Segmentation |Semantic segmentation task aims to assign a class label to each pixel in a satellite image.
|Instance Segmentation |...
|Change Detection |Change detection task identifies differences at pixel-level using time-series images. This task is used for monitoring land use changes and disaster impacts.
|Regression |Regression tasks estimate continuous variables from satellite imagery, such as biomass, surface temperature, or elevation. 
|Scene Classification |While semantic segmentation task assigns a label to pixel-level, scene classification task focuses on image-level.
|Object Detection |Object detection task aims to identify and localize discrete specific, such as buildings, vehicles and ships.
|===

// Scene classification: 画像のクラスラベル
// Object Detection: 個々の物体（クラス + 位置（BBox））

*Metrics* +
|===
|Metric Name |Description |Task Commonly Used
|Jaccard index(IoU) |the ratio between the intersection and the union of predicted and reference regions for each class. |Semantic segmentation, Change detection
|Precision | |Change Detection
|Recall | |Change Detection
|F1 | |Change Detection,  Multilabel classification
|mIoU |the average IoU across all classes. |Semantic segmentatio, Change detection
|mAP |Mean Average Precision |Object detection
|Overall Accuracy |The proportion of correctly classified samples over the total number of samples. |Scene classification
|RMSE |Root Mean Square Error(ISO/IEC CD 4213:2025) |Regression
|===

*Datasets* +
This section introduces some popular datasets to evaluate downstream tasks.
|===
|Datasets Name |Task to evaluate |Modalities |GSD |Benchmark using this dataset
|<<SpaceNet7>> |Semantic segmentation, Change detection |Planet |3m |<<PANGAEA>>, <<GEO-Bench-2>>
|<<BioMassters>> |Regression |Sentinel-1, Sentinel-2 |10m |<<PANGAEA>>, <<GEO-Bench-2>>
|<<So2Sat>> |Scene classification |Sentinel-1, Sentinel-2 |10m |<<GEO-Bench>>, <<GEO-Bench-2>>
|<<EverWatch>> |Object detection |Aerial RGB |0.1m |<<GEO-Bench-2>>
|===

=== Evaluation Process
==== Evaluation Scope
First, an evaluator needs to determine the purpose of the experiments and scope. +
For example:

* clarify the objectives of the experiments and which performance criteria is most appropriate (ex. evaluate score, inference time, robustness across modalities)
* select baseline model to compare performance

==== Dataset Selection
The second step is to select a dataset or datasets for downstream. +
Based on the objective and scope, the evaluator can choose from perspectives such as task, temporality and modality.
The temporality and modality between the downstream datasets and models do not need to be perfectly aligned, as this can be handled during model adaptation.

==== Data Adaptation
When image size or data distribution is different between pretrain dataset and downstream dataset, cropping, resizing and normalization are needed. +
If downstream dataset are scarce or lack diversity, data augmentation include horizontal and vertical flipping is important as it helps 
improve generalization.

==== Model Adaptation
Model adaptation defines how to adjust GFMs to assess its performance on downstram tasks.

*Training strategy* +
The evaluator needs to select a method to adapt a foundation model for downstream task. +
The encoder weight can be used either frozen or updated during training. +
Using frozen weights is a common evaluation protocol in self-supervised learning. 
However, this approach can be highly dependent on the pre-training task and may fail to encode information important for the downstream task.
On the other hand, it is reported that fine-tuning often mitigates this issue and frequently yields much higher generalization performance. +
In practice, <<PANGAEA>> adapts forzen encoder and <<GEO-Bench-2>> uses fine-tuning strategy.


// decoderの扱いはtaskによって異なるケースが多い。
// ・Classification: linear 
// ・segmentation and regression: plug UPerNet, UNet decoder
// ・Object Detection, Instance Segmentation:
Depending on the task, we attach a task-specific head to the encoder, such as a linear classifier for image classification, 
a UNet or UPerNet decoder for pixel-wise prediction tasks such as semantic segmentation and regression, 
or an FPN-based detection head for object detection and instance segmentation.

*Modality handling* +
// GFM は特定のセンサ／バンド構成で事前学習されているが、下流タスクのデータセットとは バンド数・種類が一致しないことが多い。
// そのためこのステップが必要。
// PANGEAはband match or zero-padding、GEO-Gench-2はセンサごとに対応を分けている。

This step is specific to the remote sensing domain, because GFMs are pre-trained with specific sensor-band that often differ from the bands available 
in downstream datasets. +
One approach suggested in <<PANGAEA>> is to use only overlapping bands and zero-padding missing ones in downstream task.
(ex. if a model is pre-trained on RGB images and the downstream dataset consists of Sentinel-2 data, we use only the RGB bands from Sentinel-2.) +
On the other hand, <<GEO-Bench-2>> offers modality-specific handling. For multispectral bands, we map model bands to the dataset bands with the nearest wavelength 
(ex., if a model pre-trained S2 Band 4, we select the dataset band whose spectral range is closest to the red band). 
For SAR, if a model cannot process radar data natively, we adapt the input by loading the VV and VH polarization bands into an RGB-like three-channel format 
(ex., using VV, VH, and VV as the model's R, G, and B channels).

*Temporality handling* +
// すべてのGFMがtemporalityを扱っている訳ではない。モデルがマルチテンポラルに対応している場合は、加工なしでdownstream datasetを入力する。
// 対応しておらず、downstream datasetがマルチテンポラルの場合は次元を揃える方法がある。
// 例えばencoderに1枚ずつ画像を入力し、encoderのoutputをconcatする方法だ。もしくは時系列次元が大きい場合などはoutputを平均したり、L-TAE, linear層を使う方法がある。

Not all GFMs are pre-trained using multi-temporal datasets. +
If a model natively supports multi-temporal, multi-temporal downstream datasets can be fed directly to the encoder-decoder pipeline.
However, to enable models that can only handle single images to be trained on multi-temporal datasets, each timestamp is passed independently through 
the encoder and the output features are aggregated over time before passing decoder.
Common temporal aggregation methods include concatenation, mean pooling, a naive linear mapping that compresses the temporal dimension, and 
temporal-attention modules such as L-TAE suggested by <<Vivien Sainte Fare Garnot, Loic Landrieu>>.

==== Training & Inference
// ・学習にあたって、lr、batch_size,epoch数、weghti decayなどのhyperparameterを設定する必要があります
// ・探索を行う場合はフェアな評価のため探索範囲を固定します
// ・これらは最大何トライアルまでかを固定し、ばらつきを考慮するためRepeated Testの回数も統一します。
For a fair comparison, the model shoud be fine-tuned with an identical set of hyperparameters such as learning rate, batch size, number of epochs, 
and weight decay.
For hyperparameter optimization, the search space and the maximum number of trials should be fixed. +
The model is trained using the specified hyperparameter values, and repeated tests are conducted to account for variability in training outcomes. The final performance is then evaluated using the chosen metric across all repeated runs.

// and the number of repeated runs are standardized to account for variability across random seeds. This consistent setup prevents uneven tuning across models and maintains the reliability of the evaluation.

Using these setting, the model is trained and subsequently used for inference.

==== Evaluating
Assess model performance against the objectives.

=== Challenges and Future Directions
// ①汎化性能ない
// GFMは汎化性能が不足している。 
// RSデータは地域・時間・センサなどが異なるとデータ分布が大きく異なる。 
// 本来その多様性に対応できるようなモデルが必要だが（ex. Europeにも南米にも対応できるモデル、Landsat-8にもSentinel-2にも対応できるモデル）、trainingが偏っており、ドメインシフトが起こってしまうのが現状。 
// →(1) datasetをセンサ、地域など複数の視点で評価するしくみを確立する（地域レベルではうまく評価できているけど、センサレベルではうまくできていないなど）
// →(2) Optical ↔ SARなどドメインが異なるもの同士の距離を定量化し、どのデータをどれだけ学習に使うべきかを決める（Data Scheduling）
// ②ブラックボックス化
// GFMは環境問題など重要なタスクで利用されているため、信頼性、透明性、説明責任を確保するために、それらの解釈可能性を高めることが不可欠である。
// →(1)説明可能なAI（XAI）をRS分野でも活用し、モデルの出力を、地理的根拠（どこ）と物理的根拠（なぜその値になる）に結びつけて説明できるようにする

// ③High-resolution RS dataは商用利用しづらい、国家主権、プライバシーにより共有が制限される（これは書かなかった）

As introduced, studies on GFMs for satellite imagery are rapidly advancing.
However, a few limitations have been identified. +

*Generalization* +
First, GFMs still struggle with generalization across domains. EO data are highly heterogeneous because they are collected from different sensors, regions, and temporality. 
This heterogeneity leads to substantial shifts in data distribution across domains, meaning that the statistical properties of the training data often differ 
from those of the test data. Such domain shifts make it hard for the model to learn stable features, which lowers its generalization performance. +
One idea to address this issue is to establish hierarchical evaluation frameworks. They assess datasets across multiple aspects (e.g., region, sensor) to reveal where coverage or bias is insufficient. <<Survey>>, <<Foundation>>
// domains: local, temporal, sensor

*Interpretability* +
EO data are used in the areas such as environmental monitoring, climate change assessment, and disaster response, where decisions can have serious societal and ecological impacts. 
Because of this, these applications require transparency, traceability, robustness.
However, GFMs still provide limited interpretability. This problem becomes even more significant when they integrate heterogeneous sensor modalities, 
because the fusion process is complex and the internal reasoning steps are hard to visualize or interpret. +
A promising direction for improving interpretability is applying XAI in remote sensing, enabling users to connect model outputs with geospatial evidence 
(e.g., the specific pixels/regions responsible for the output) and physical evidence (e.g., sensor-measured signals such as reflectance spectra or SAR backscatter).
<<Survey>>, <<Foundation>>
